{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\yolo\\yolov8-face\\ultralytics\\nn\\tasks.py:362: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(weight, map_location='cpu')  # load\n",
      "Ultralytics YOLOv8.0.17  Python-3.11.7 torch-2.4.1+cpu CPU\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique faces detected: 127\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Load YOLOv8 face detection model\n",
    "model = YOLO('yolov8n-face.pt')\n",
    "\n",
    "# Open video file\n",
    "video_path = 'test_video.mp4'  # Input video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define VideoWriter to save output video\n",
    "output_path = 'output_2.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Define minimum distance to consider two faces the same\n",
    "min_distance = 50  # Adjust based on video resolution and expected movement\n",
    "\n",
    "# List to store unique face centers\n",
    "unique_faces = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run inference on the frame with YOLOv8 face detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # List to store the centers of detected faces in this frame\n",
    "    current_faces = []\n",
    "\n",
    "    for r in results:\n",
    "        boxes = r.boxes  # Extract bounding boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates\n",
    "            conf = box.conf  # Confidence score\n",
    "            if conf >= 0.25:  # Confidence threshold for face detection\n",
    "                # Calculate the center of the bounding box\n",
    "                center_x = (x1 + x2) // 2\n",
    "                center_y = (y1 + y2) // 2\n",
    "                current_faces.append((center_x, center_y))\n",
    "\n",
    "                # Draw bounding box around the face\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    # Compare detected faces in this frame with unique faces tracked\n",
    "    for face in current_faces:\n",
    "        distances = [distance.euclidean(face, u_face) for u_face in unique_faces]\n",
    "        if len(distances) == 0 or min(distances) > min_distance:\n",
    "            # If no close match is found, consider it a new unique face\n",
    "            unique_faces.append(face)\n",
    "\n",
    "    # Write the frame with detections to the output video\n",
    "    out.write(frame)\n",
    "\n",
    "    # Display the frame (optional)\n",
    "    cv2.imshow('YOLOv8 Face Detection', frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and writer, close windows\n",
    "cap.release()\n",
    "out.release()  # Save the output video\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Output the total number of unique faces detected\n",
    "print(f\"Number of unique faces detected: {len(unique_faces)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
